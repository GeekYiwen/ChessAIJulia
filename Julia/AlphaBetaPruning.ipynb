{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(read(open(\"style.css\"), String))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha-Beta Pruning\n",
    "The Alpha-Beta Pruning algorithm works by recursively evaluating nodes\n",
    "in a game tree. The Alpha-Beta Pruning algorithm can be described as an improved version\n",
    "of the MiniMax algorithm. The MiniMax algorithm evaluates all possible\n",
    "game outcomes to find the best move for a player, which is describe in notebook MiniMax. \n",
    "\n",
    "However, this algorithm works by maintaining two values, alpha and beta, which\n",
    "represent the lower and upper boundary of values that each player can achieve. By comparing these values with previously evaluated nodes,\n",
    "alpha-beta pruning can determine which branches of the tree can be\n",
    "pruned without affecting the final result. So it is possible to prune\n",
    "branches that cannot lead to a better outcome. This reduces the search\n",
    "space, resulting in faster computation times and allowing the algorithm\n",
    "to search deeper into the game tree. \n",
    "\n",
    "The algorithm can be summarzied using the following equations. \n",
    "\n",
    "$ s \\in states $\n",
    "\n",
    "1. $\\alpha \\leq maxValue(s) \\leq \\beta \\longrightarrow alphaBetaMax(s, \\alpha, \\beta) = maxValue(s)$\n",
    "1. $maxValue(s) \\lt \\alpha \\longrightarrow alphaBetaMax(s, \\alpha, \\beta) \\leq \\alpha$ \n",
    "1. $\\beta \\lt maxValue(s) \\longrightarrow \\beta \\leq alphaBetaMax(s, \\alpha, \\beta)$ \n",
    "\n",
    "\n",
    "1. $\\alpha \\leq minValue(s) \\leq \\beta \\longrightarrow alphaBetaMax(s, \\alpha, \\beta) = minValue(s)$ \n",
    "1. $minValue(s) \\lt \\alpha \\longrightarrow alphaBetaMax(s, \\alpha, \\beta) \\leq \\alpha $ \n",
    "1. $\\beta \\lt minValue(s)\\longrightarrow \\beta \\leq alphaBetaMax(s, \\alpha, \\beta)$ \n",
    "\n",
    "A comprehensive validation of this technique is presented in the work of Donald E. Knuth and Ronald W. Moore \\cite{Knuth1975}, which provides further insight into the effectiveness of this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "# Pkg.add(\"Chess\")\n",
    "using Chess\n",
    "using Random\n",
    "\n",
    "# Pkg.add(\"NBInclude\")\n",
    "using NBInclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nbinclude(\"EvaluatePosition.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nbinclude(\"Memoization.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nbinclude(\"AdvancedBoard.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaBetaMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `alphaBetaMax_noMem` function takes in 3 arguments and 2 optional arguments.\n",
    "\n",
    "1. `aBoard::AdvBoard` is a chess board in the current state\n",
    "1. `depth` is the number of halfmoves the engine should analyze before terminating \n",
    "1. `alpha` is optional and is default to -Infinity. Alpha is a minimal value that has been calculated during the recursive process\n",
    "1. `beta`  is optional and is default to Infinity . Beta is a maximal value that has been calculated during the recursive process\n",
    "\n",
    "The function returns the maximal centipawn evaluation of the current position for the player playing white where both players have played the optimal moves according to the algorithm and terminating after the given depth. This function does not use Memoization.\n",
    "\n",
    "We will discuss the requirements for the alphaBetaMax function and how it satisfies these requirements:\n",
    "- One of the requirements for the alphaBetaMax function is that it should compute the same result as the maxValue function when the maximum value of a node is between α and β. This means that:\n",
    "\n",
    "$$\\alpha \\leq maxValue(s) \\leq \\beta \\longrightarrow alphaBetaMax(s, \\alpha, \\beta) = maxValue(s)$$\n",
    "\n",
    "This requirement ensures that the alphaBetaMax function does not prune any nodes that could potentially lead to the optimal solution.\n",
    "- Another requirement for the alphaBetaMax function is that it should return a value less than or equal to α when the maximum value of a node is less than α. This means that:\n",
    "\n",
    "$$maxValue(s) \\lt \\alpha \\longrightarrow alphaBetaMax(s, \\alpha, \\beta) \\leq \\alpha$$\n",
    "\n",
    "- The last requirement for the alphaBetaMax function is that it should return a value greater than or equal to β when the maximum value of a node is greater than β. This means that:\n",
    "\n",
    "$$\\beta \\lt maxValue(s) \\longrightarrow \\beta \\leq alphaBetaMax(s, \\alpha, \\beta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function alphaBetaMax_noMem(aBoard::AdvBoard, depth::Int64, alpha::Int64=-200000, beta::Int64=200000)::Int64\n",
    "    if isterminal(aBoard.state)\n",
    "        return terminal_evaluation(aBoard) - depth\n",
    "    end\n",
    "    if depth == 0\n",
    "        return aBoard.score\n",
    "    end\n",
    "    for move in moves(aBoard.state)\n",
    "        Undo = domoveAdv!(aBoard, move)\n",
    "        value = alphaBetaMin_noMem(aBoard, depth - 1, alpha, beta)\n",
    "        undomoveAdv!(aBoard, Undo)\n",
    "        if value >= beta\n",
    "            return value\n",
    "        end\n",
    "        alpha = max(alpha, value)\n",
    "    end\n",
    "    return alpha\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaBetaMin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `alphaBetaMin_noMem` function takes in 3 arguments and 2 optional arguments.\n",
    "\n",
    "1. `aBoard::AdvBoard` is a chess board in the current state\n",
    "1. `depth` is the number of halfmoves the engine should analyze before terminating \n",
    "1. `alpha` is optional and is default to -Infinity. Alpha is a minimal value that has been calculated during the recursive process\n",
    "1. `beta`  is optional and is default to Infinity . Beta is a maximal value that has been calculated during the recursive process\n",
    "\n",
    "The function returns the minimal centipawn evaluation of the current position for the player playing black where both players have played the optimal moves according to the algorithm and terminating after the given depth. This function does not use Memoization.\n",
    "\n",
    "Similar to \"alphaBetaMax\", there are certain requirements that must be met for \"alphaBetaMin\" to ensure a correct approximation:\n",
    "- If \"maxValue(s)\" is between the values of α and β, \"alphaBetaMin\" computes the same value as \"minValue(s)\", i.e. \n",
    "\n",
    "$$\\alpha \\leq minValue(s) \\leq \\beta \\longrightarrow alphaBetaMax(s, \\alpha, \\beta) = minValue(s)$$ \n",
    "\n",
    "This means that \"alphaBetaMin\" returns the exact minimum value if it is between α and β.\n",
    "- If \"minValue(s)\" is less than α, the value returned by \"alphaBetaMin\" must be less than or equal to α, i.e. \n",
    "\n",
    "$$minValue(s) \\lt \\alpha \\longrightarrow alphaBetaMax(s, \\alpha, \\beta) \\leq \\alpha $$\n",
    "\n",
    "This means that \"alphaBetaMin\" returns a value that is not smaller than α if the minimum value is smaller than α. This optimizes the algorithm, as there is no point in examining further moves if the minimum value is already smaller than the best known value.\n",
    "- Similarly, if \"minValue(s)\" is greater than β, the value returned by \"alphaBetaMin\" must be greater than or equal to β, i.e.\n",
    "\n",
    "$$\\beta \\lt minValue(s)\\longrightarrow \\beta \\leq alphaBetaMax(s, \\alpha, \\beta)$$\n",
    "\n",
    "This means that \"alphaBetaMin\" returns a value that is not larger than β if the minimum value is greater than β. This is another optimization of the algorithm, as there is no point in examining further moves if the minimum value is already greater than the best known value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function alphaBetaMin_noMem(aBoard::AdvBoard, depth::Int64, alpha::Int64=-200000, beta::Int64=200000)::Int64\n",
    "    if isterminal(aBoard.state)\n",
    "        return terminal_evaluation(aBoard) + depth\n",
    "    end\n",
    "    if depth == 0\n",
    "        return aBoard.score\n",
    "    end\n",
    "    for move in moves(aBoard.state)\n",
    "        Undo = domoveAdv!(aBoard, move)\n",
    "        value = alphaBetaMax_noMem(aBoard, depth - 1, alpha, beta)\n",
    "        undomoveAdv!(aBoard, Undo)\n",
    "        if value <= alpha\n",
    "            return value\n",
    "        end\n",
    "        beta = min(beta, value)\n",
    "    end\n",
    "    return beta\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha-Beta-Pruning function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `alphaBetaPruning_noMem` function takes in 3 arguments\n",
    "\n",
    "1. `aBoard::AdvBoard` is a chess board in the current state\n",
    "1. `depth` is the number of halfmoves the engine should analyze before terminating\n",
    "\n",
    "The function returns the best value and the best move the moving player can play in the current position. It calls the alpha-beta-pruning algorithm. If multiple moves are found which result in the best evaluation a random move will be chosen. This function does not use Memoization.\n",
    "\n",
    "\n",
    "The `alphaBetaPruning_noMem` function calls the alpha-beta pruning algorithm. It first generates a list of possible moves from the current state. Then it iterates over each move, evaluates the heuristic of the current state using the evaluate_move function, and calls the alphaBetaMin_noMem or alphaBetaMax_noMem function depending on which player is moving. The algorithm prunes parts of the game tree that cannot lead to a better score than the current alpha or beta value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function alphaBetaPruning_noMem(aBoard::AdvBoard, depth::Int64)::Tuple{Int64, Move}\n",
    "    next_moves = moves(aBoard.state)\n",
    "    BestMoves = []\n",
    "    if sidetomove(aBoard.state) == WHITE\n",
    "        bestVal = alphaBetaMax_noMem(aBoard, Int(depth))\n",
    "        for move in next_moves \n",
    "            Undo = domoveAdv!(aBoard, move)\n",
    "            if alphaBetaMin_noMem(aBoard, depth - 1) == bestVal\n",
    "                append!(BestMoves, [move])\n",
    "            end\n",
    "            undomoveAdv!(aBoard, Undo)\n",
    "        end\n",
    "    elseif sidetomove(aBoard.state) == BLACK\n",
    "        bestVal = alphaBetaMin_noMem(aBoard, depth)\n",
    "        for move in next_moves \n",
    "            Undo = domoveAdv!(aBoard, move)\n",
    "            if alphaBetaMax_noMem(aBoard, depth - 1) == bestVal\n",
    "                append!(BestMoves, [move])\n",
    "            end\n",
    "            undomoveAdv!(aBoard, Undo)\n",
    "        end\n",
    "    end\n",
    "    BestMove = rand(BestMoves)\n",
    "    return bestVal, BestMove\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha-beta-Pruning with Memoization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transposition tables store previously computed positions and their associated values, allowing the algorithm to avoid redundant computations and speed up the search process \\cite{Marsland1982} (p. 8). \n",
    "In this research paper, the transposition tables are referred to as memoization, and the values are stored in a cache. The cache is implemented as a dictionary, where the key is a hash value representing a state, and the value is a tuple containing a flag, the evaluation of the state, and the remaining search depth. A detailed description of the cache can be found in the Memoization Notebook section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nbinclude(\"Memoization.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlphaBetaMax function with Memoization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `alphaBetaMax` function takes in 4 arguments and 2 optional arguments.\n",
    "\n",
    "1. `aBoard::AdvBoard` is a chess board in the current state\n",
    "1. `depth` is the number of halfmoves the engine should analyze before terminating \n",
    "1. `cache` is a dictionary which stores the calculated values\n",
    "1. `alpha` is optional and is default to -Infinity. Alpha is a minimal value that has been calculated during the recursive process\n",
    "1. `beta`  is optional and is default to Infinity . Beta is a maximal value that has been calculated during the recursive process\n",
    "\n",
    "The function returns the maximal centipawn evaluation of the current position for the player playing white where both players have played the optimal moves according to the algorithm and terminating after the given depth. This function does use Memoization meaning it saves and uses calculated values stored the `Cache`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function alphaBetaMax(aBoard::AdvBoard, depth::Int64, \n",
    "                      cache::Dict{UInt64, Tuple{String, Int64, Int64}}, alpha::Int64=-200000, beta::Int64=200000)::Int64\n",
    "    if isterminal(aBoard.state)\n",
    "        return terminal_evaluation(aBoard) - depth\n",
    "    end\n",
    "    if depth == 0\n",
    "        return aBoard.score\n",
    "    end\n",
    "    for move in moves(aBoard.state)\n",
    "        Undo = domoveAdv!(aBoard, move)\n",
    "        value = evaluate(aBoard, alphaBetaMin, depth - 1, cache, alpha, beta)\n",
    "        undomoveAdv!(aBoard, Undo)\n",
    "        if value >= beta\n",
    "            return value\n",
    "        end\n",
    "        alpha = max(alpha, value)\n",
    "    end\n",
    "    return alpha\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlphaBetaMin function with memoization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Alpha-Beta-Min function takes in 4 arguments and 2 optional arguments.\n",
    "\n",
    "1. `aBoard::AdvBoard` is a chess board in the current state\n",
    "1. `depth` is the number of halfmoves the engine should analyze before terminating \n",
    "1. `cache` is a dictionary which stores the calculated values\n",
    "1. `alpha` is optional and is default to -Infinity. Alpha is a minimal value that has been calculated during the recursive process\n",
    "1. `beta`  is optional and is default to Infinity . Beta is a maximal value that has been calculated during the recursive process\n",
    "\n",
    "The function returns the minimal centipawn evaluation of the current position for the player playing black where both players have played the optimal moves according to the algorithm and terminating after the given depth. This function does use Memoization meaning it saves and uses calculated values stored the `Cache`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function alphaBetaMin(aBoard::AdvBoard, depth::Int64, \n",
    "                      cache::Dict{UInt64, Tuple{String, Int64, Int64}}, alpha::Int64=-200000, beta::Int64=200000)::Int64\n",
    "    if isterminal(aBoard.state)\n",
    "        return terminal_evaluation(aBoard) + depth\n",
    "    end\n",
    "    if depth == 0\n",
    "        return aBoard.score\n",
    "    end\n",
    "    for move in moves(aBoard.state)\n",
    "        Undo = domoveAdv!(aBoard, move)\n",
    "        value = evaluate(aBoard, alphaBetaMax, depth - 1, cache ,alpha, beta)\n",
    "        undomoveAdv!(aBoard, Undo)\n",
    "        if value <= alpha\n",
    "            return value\n",
    "        end\n",
    "        beta = min(beta, value)\n",
    "    end\n",
    "    return beta\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `alphaBetaPruning` function takes in 3 arguments and 1 optional argument.\n",
    "\n",
    "1. `aBoard::AdvBoard` is a chess board in the current state\n",
    "1. `depth` is the number of halfmoves the engine should analyze before terminating\n",
    "1. `cache` is optional and is default empty dictionary. Cache is a dictionary which stores the calculated values\n",
    "\n",
    "The function returns the best value and the best move the moving player can play in the current position. It calls the alpha-beta-pruning algorithm. If multiple moves are found which result in the best evaluation a random move will be chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function alphaBetaPruning(aBoard::AdvBoard, depth::Int64,\n",
    "                          cache::Dict{UInt64, Tuple{String, Int64, Int64}} = initCache())::Tuple{Int64, Move}\n",
    "    next_moves = moves(aBoard.state)\n",
    "    BestMoves = []\n",
    "    if sidetomove(aBoard.state) == WHITE\n",
    "        bestVal = evaluate(aBoard, alphaBetaMax, depth, cache, -200000, 200000)\n",
    "        for move in next_moves\n",
    "            Undo = domoveAdv!(aBoard, move)\n",
    "            if evaluate(aBoard, alphaBetaMin, depth-1, cache,-200000, 200000) == bestVal\n",
    "                append!(BestMoves, [move])\n",
    "            end\n",
    "            undomoveAdv!(aBoard, Undo)\n",
    "        end\n",
    "    elseif sidetomove(aBoard.state) == BLACK\n",
    "        bestVal = evaluate(aBoard, alphaBetaMin, depth, cache,-200000, 200000)\n",
    "        for move in next_moves \n",
    "            Undo = domoveAdv!(aBoard, move)\n",
    "            if evaluate(aBoard, alphaBetaMax, depth-1, cache,-200000, 200000) == bestVal\n",
    "                append!(BestMoves, [move])\n",
    "            end\n",
    "            undomoveAdv!(aBoard, Undo)\n",
    "        end\n",
    "    end\n",
    "    BestMove = rand(BestMoves)\n",
    "    return bestVal, BestMove\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
